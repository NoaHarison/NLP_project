# -*- coding: utf-8 -*-
"""TTR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rD5mz6DhZjFcmG1WDdZA_7Ha4HtgXULw
"""

pip install nltk

import nltk
nltk.download('punkt')

#רק בדיקה
import string
import nltk

nltk.download('punkt')

def remove_punctuation(text):
    translator = str.maketrans("", "", string.punctuation)
    text_without_punctuations = text.translate(translator)
    return text_without_punctuations

text = "הכלב שחקן חשוב בבית. הוא אוהב לשחק בחצר ולשנות לילדים."

# מסירת סימני פיסוק מהטקסט
cleaned_text = remove_punctuation(text)

# פיצול לאסימונים
tokens = nltk.word_tokenize(cleaned_text)

# Type-Token Ratio
ttr_result = len(set(tokens)) / len(tokens)
print("Type-Token Ratio:", ttr_result)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!ls drive/MyDrive

import numpy as np#מאפשר עיבוד מהיר של מערכים מרובי ממדים ופעולות מתמטיות עליהם
import pandas as pd#משמש לניתוח ועיבוד נתונים בטבלאות, מאגרי נתונים וקבצי CSV
import re#מספק כלים לעיבוד ופעולות על טקסט באמצעות פונקציות רגולריות.
import tensorflow as tf# ספריית למידת מכונה נרחבת, תחת המודול tf.keras המאפשרת בניית רשתות עם שכבות שונות, אופטימיזציה ועוד.
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow_hub as hub

!find drive/MyDrive

!ls drive/MyDrive/Colab_Notebooks/Noa/data.zip

pip install spacy

!unzip drive/MyDrive/Colab_Notebooks/Noa/data.zip

!python -m spacy download en_core_web_sm

#בדיקה
import string
import nltk
import spacy
import re

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def remove_punctuation_and_non_english(text, nlp):
    translator = str.maketrans("", "", string.punctuation)
    text_without_punctuations = text.translate(translator)
    tokens = nltk.word_tokenize(text_without_punctuations)

    # בדיקת קיומן של המילים במילון של spaCy
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    return english_tokens

# טעינת המודל של spaCy
language_model = "en_core_web_sm"
nlp = spacy.load(language_model)

text = "I am meeeeeeee, and so you sooooooo so"
english_tokens = remove_punctuation_and_non_english(text, nlp)

# Type-Token Ratio
ttr_result = len(set(english_tokens)) / len(english_tokens)
print("Type-Token Ratio:", ttr_result)

#בדיקה
import string
import nltk
import spacy

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(sentence, nlp):
    # Convert to lowercase
    sentence_lower = sentence.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    sentence_without_punctuations = sentence_lower.translate(translator)

    # Tokenize the sentence
    tokens = nltk.word_tokenize(sentence_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words

def process_sentence(sentence, nlp):
    # Preprocess and count words
    total_words, unique_english_words = preprocess_and_count_words(sentence, nlp)

    # Calculate TTR
    ttr_result = unique_english_words / total_words if total_words > 0 else 0

    # Print the results
    print(f"Sentence: {sentence}")
    print(f"Total Words: {total_words}")
    print(f"Unique English Words: {unique_english_words}")
    print(f"TTR: {ttr_result}")

# Example usage:
nlp = spacy.load("en_core_web_sm")

sentence = "Hello i am am hello jfgbjnfgkjngk gjjghnujuyu am."
process_sentence(sentence, nlp)

#בדיקההההההה
import string
import nltk
import spacy

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)  # Update to the number of words that are preserved
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_sentence(sentence, nlp):
    # Preprocess and count words
    total_words, unique_english_words, _ = preprocess_and_count_words(sentence, nlp)

    # Calculate TTR
    ttr_result = unique_english_words / total_words if total_words > 0 else 0

    # Print the results
    print(f"Sentence: {sentence}")
    print(f"Total Words: {total_words}")
    print(f"Unique English Words: {unique_english_words}")
    print(f"TTR: {ttr_result}")

# Example usage:
nlp = spacy.load("en_core_web_sm")

sentence = "Hello i am am hello jfgbjnfgkjngk gjjghnujuyu am."
process_sentence(sentence, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
             if 'finished' in file_info.filename and 'Braunwald' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
             if 'finished' in file_info.filename and 'Brown/Adam' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
             if 'finished' in file_info.filename and 'Brown/Eve' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'Brown/Sarah' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
             if 'finished' in file_info.filename and 'ComptonPater/Julia' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
             if 'finished' in file_info.filename and 'ComptonPater/Sean' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'ComptonPater/Trevor' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Goad/Julia/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Goad/Sonya/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'Inkelas/E/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/Alex/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/Ethan/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/Lily/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/Naima/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/Violet/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)



import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
               if 'finished' in file_info.filename and '/Providence/William/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'Sachs/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'Suppes/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, english_tokens = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                    'English Tokens': english_tokens
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Export DataFrame to Excel
    excel_path = 'drive/MyDrive/Colab_Notebooks/Noa/data_analysis_result_formatted.xlsx'
    df.to_excel(excel_path, index=False)
    print(f'Data has been exported to Excel: {excel_path}')

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)

import string
import nltk
import spacy

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(sentence, nlp):
    # Convert to lowercase
    sentence_lower = sentence.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    sentence_without_punctuations = sentence_lower.translate(translator)

    # Tokenize the sentence
    tokens = nltk.word_tokenize(sentence_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words

# Example usage:
nlp = spacy.load("en_core_web_sm")

sentence = "Hello i am am hello jfgbjnfgkjngk."
total_words, unique_english_words = preprocess_and_count_words(sentence, nlp)

# Calculate TTR
ttr_result = unique_english_words / total_words

print(f"Total Words: {total_words}")
print(f"Unique English Words: {unique_english_words}")
print(f"TTR: {ttr_result}")

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Braunwald_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Braunwald - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Adam_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Adam - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Eve_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Eve - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Sarah_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Sarah - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Julia_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Julia - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Sean_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Sean - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Trevor_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Trevor - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Julia_2_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Julia_2 - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Sonya_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Sonya - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Inkelas_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Inkelas - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Alex_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Alex - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Ethan_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Ethan - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Lily_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Lily - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Naima_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Naima - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Violet_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Violet - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/william_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('William - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Sachs_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Sachs - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# קרא את הקובץ האקסל
excel_path = 'drive/MyDrive/Colab_Notebooks/All_data_for_kids/length_and_non_english/Suppes_data_analysis_result_formatted.xlsx'
df = pd.read_excel(excel_path)

# יצירת גרף Scatter
plt.scatter(df['age'], df['TTR'])

# הגדרות נוספות לגרף
plt.title('Suppes - Scatter Plot - Age vs TTR')
plt.xlabel('age')
plt.ylabel('TTR')

# הצגת הגרף
plt.show()

import string
import nltk
import spacy
import pandas as pd
import os
import zipfile
import matplotlib.pyplot as plt

def is_word_in_vocab(word, nlp):
    return word.lower() in nlp.vocab.strings

def preprocess_and_count_words(content, nlp):
    # Convert to lowercase
    content_lower = content.lower()

    # Remove punctuation
    translator = str.maketrans("", "", string.punctuation)
    content_without_punctuations = content_lower.translate(translator)

    # Tokenize the content
    tokens = nltk.word_tokenize(content_without_punctuations)

    # Keep only English words
    english_tokens = [word for word in tokens if is_word_in_vocab(word, nlp)]

    # Count the total number of words and unique words
    total_words = len(english_tokens)
    unique_english_words = len(set(english_tokens))

    return total_words, unique_english_words, english_tokens

def process_zip(zip_path, target_folder, nlp):
    # List to store data
    data_list = []

    # Open the ZIP file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Iterate over each file in the ZIP
        for file_info in zip_ref.infolist():
            # Check if the filename meets the criteria
              if 'finished' in file_info.filename and 'Suppes/finished' in file_info.filename:
                # Extract the file to the target folder
                zip_ref.extract(file_info, target_folder)
                extracted_file_path = os.path.join(target_folder, file_info.filename)

                # Read the content of the extracted file
                with open(extracted_file_path, 'r', encoding='utf-8') as file:
                    content = file.read()

                # Preprocess and count words
                total_words, unique_english_words, _ = preprocess_and_count_words(content, nlp)

                # Calculate TTR
                ttr_result = unique_english_words / total_words if total_words > 0 else 0

                # Append the data to the list
                data_list.append({
                    'File': file_info.filename,
                    'Total Tokens': total_words,
                    'Unique English Tokens': unique_english_words,
                    'TTR': ttr_result,
                })

                # Remove the extracted file
                os.remove(extracted_file_path)

    # Create a DataFrame from the list of data
    df = pd.DataFrame(data_list)

    # Sorting the DataFrame by 'File' (assuming 'File' represents the chronological order)
    df = df.sort_values(by='File')

    # Scatter plot
    plt.scatter(df.index, df['TTR'])
    plt.title('Scatter plot of TTR over time')
    plt.xlabel('File Index (Chronological Order)')
    plt.ylabel('TTR')
    plt.show()

# Example usage:
nlp = spacy.load("en_core_web_sm")

# Path to the ZIP file
zip_path = 'drive/MyDrive/Colab_Notebooks/Noa/data.zip'

# Target folder to extract files
target_folder = 'path/to/target/folder'

# Process the ZIP file
process_zip(zip_path, target_folder, nlp)