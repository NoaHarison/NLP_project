import pandas as pd
import numpy as np
import random
import gzip
import json
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import evaluate

# Step 1: Loading the compressed dataset and extracting reviews
file_path = '/workspace/Levona/Noa/Electronics.jsonl.gz'

# Lists to store review texts and ratings
texts = []
ratings = []
example_printed = False  # Flag to print only one valid example

# Open the compressed file and read its lines
with gzip.open(file_path, 'rt', encoding='utf-8') as f:
    lines = f.readlines()

# Function to check if a review meets the condition of 3 sentences and 45 words total
def is_valid_review(text):
    sentences = text.split('.')
    long_sentences = [s for s in sentences if len(s.split()) >= 15]
    return len(long_sentences) >= 3

# Read the lines and filter by the condition of 3 sentences and 45 words total
for line in lines:
    json_obj = json.loads(line.strip())
    if 'text' in json_obj and 'rating' in json_obj:
        texts.append(json_obj['text'].lower())  # Convert review text to lowercase
        ratings.append(json_obj['rating'])  # Collect rating

        # Print one example that meets the criteria
        if not example_printed:
            print("Example review that meets the criteria:")
            print(json_obj['text'])
            example_printed = True  # Only print one example

# Create a DataFrame from the texts and ratings
df = pd.DataFrame({
    'text': texts,
    'rating': ratings
})

# Step 2: Show the number of rows in each set
print(f"Total sample size: {len(df)}")
print(df['rating'].value_counts())  # Verify the balance of ratings

# Step 3: Split the data into training (80%), validation (10%), and test (10%) sets
train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(
    df['text'].tolist(),
    df['rating'].tolist(),
    test_size=0.1,  # 10% for testing
    random_state=42
)

# Split the remaining 90% into 80% training and 10% validation
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_val_texts,
    train_val_labels,
    test_size=0.1111,  # 10% of the remaining 90% (0.1 / 0.9 = 0.1111)
    random_state=42
)

# Step 4: Tokenize the texts for training, validation, and testing
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# Step 5: Define a custom dataset class
class TextClassificationDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.labels)

# Create dataset objects for training, validation, and testing
train_dataset = TextClassificationDataset(train_encodings, train_labels)
val_dataset = TextClassificationDataset(val_encodings, val_labels)
test_dataset = TextClassificationDataset(test_encodings, test_labels)

# Step 6: Define the data collator
data_collator = DataCollatorWithPadding(tokenizer)

# Step 7: Load the BERT model for regression
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", problem_type="regression", num_labels=1)

# Step 8: Define training arguments
training_args = TrainingArguments(
    output_dir="/home/access/Projects/Levona/Noa",  # Output directory for saving the model
    overwrite_output_dir=True,  # Overwrite the content of the output directory
    learning_rate=5e-5,  # Learning rate
    per_device_train_batch_size=32,  # Batch size for training
    per_device_eval_batch_size=32,  # Batch size for evaluation
    num_train_epochs=13,  # Number of epochs
    weight_decay=0.1,  # Weight decay
    eval_strategy="epoch",  # Evaluate at the end of each epoch
    save_strategy="epoch",  # Save the model at the end of each epoch
    load_best_model_at_end=True,  # Load best model at the end of training
    push_to_hub=False,  # Don't push the model to the hub
    logging_steps=20,  # Logging steps
)

# Step 9: Define the evaluation metric
metric = evaluate.load("mse")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.squeeze()
    mse = ((predictions - labels) ** 2).mean().item()
    print(f"MSE at the end of epoch: {mse:.4f}")  # Print the MSE at the end of each epoch
    return {"mse": mse}

# Step 10: Define a custom trainer
class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(-1)
        loss_fn = torch.nn.functional.mse_loss
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

# Step 11: Instantiate the trainer
trainer = MyTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,  # Set the validation dataset
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Step 12: Train the model
trainer.train()

# Step 13: Save the model
# The model will be saved automatically at the end of each epoch to the output_dir

# Step 14: Evaluate the model on the test set
test_predictions = trainer.predict(test_dataset)
test_preds = test_predictions.predictions.squeeze()

# Calculate regression metrics for the test set
test_true = test_labels
test_mse = mean_squared_error(test_true, test_preds)
test_rmse = np.sqrt(test_mse)
test_mae = mean_absolute_error(test_true, test_preds)
test_r2 = r2_score(test_true, test_preds)

print(f"Test MSE: {test_mse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Test MAE: {test_mae:.4f}")
print(f"Test RÂ²: {test_r2:.4f}")

# Plotting the true vs predicted values and saving the plot as a PNG file
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(test_true, test_preds, alpha=0.5)
plt.plot([min(test_true), max(test_true)], [min(test_true), max(test_true)], color='red', linestyle='--')  # Line for perfect predictions
plt.xlabel('True Ratings')
plt.ylabel('Predicted Ratings')
plt.title('True vs Predicted Ratings')
plt.grid(True)

# Save the figure as a PNG file
plt.savefig('true_vs_predicted.png')
