# -*- coding: utf-8 -*-
"""huggingFace_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GTD1l2ZHdHR-FJnc9tn6xYdeZd8gFveH
"""

from google.colab import drive
drive.mount('/content/drive',  force_remount=True)

#ספריות
import numpy as np#מאפשר עיבוד מהיר של מערכים מרובי ממדים ופעולות מתמטיות עליהם
import pandas as pd#משמש לניתוח ועיבוד נתונים בטבלאות, מאגרי נתונים וקבצי CSV
import re#מספק כלים לעיבוד ופעולות על טקסט באמצעות פונקציות רגולריות.
import tensorflow as tf# ספריית למידת מכונה נרחבת, תחת המודול tf.keras המאפשרת בניית רשתות עם שכבות שונות, אופטימיזציה ועוד.
#from tensorflow.keras.layers import Dense, Input, Dropout
#from tensorflow.keras.optimizers import Adam
#from tensorflow.keras.models import Model
#from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow_hub as hub#מאפשר לטעון מודלים כבר מאומנים מספרטים של TensorFlow Hub כדי להשתמש בהם בקלות במודלים הנוכחיים.
from transformers import Trainer #יכול להיות זה לא מספיק, לבדוק
#from nltk.corpus import stopwords
#from nltk.stem.porter import PorterStemmer

#import tokenization

!pip install transformers

!ls drive/MyDrive

!find drive/MyDrive

!unzip drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/spam_or_not/archive.zip

import pandas as pd
from sklearn.utils import shuffle
import matplotlib.pyplot as plt

# קריאת המידע
df = pd.read_csv('drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/spam_or_not/spam_ham_dataset.csv', names=[ 'num','label','text','label_num'])
df = shuffle(df)
print(df['label_num'].dtype)
df['label_num'] = pd.to_numeric(df['label_num'], errors='coerce')
print(df['label_num'].dtype)
df['label_num'] = df['label_num'].fillna(0).astype(int)
print(df['label_num'].dtype)


# יצירת הטבלה החדשה
#df_3cols = pd.DataFrame({'label': df['label'], 'text': df['text'], 'label_num': df['label']})
df_2cols = df[['label_num', 'text']]



text = df.text.values
labels = df.label_num.values
print(len(text))
print(df.label_num.value_counts())

from sklearn.model_selection import train_test_split#ייבוא פונקציה שתפקידה לחלק נתונים לקבוצות של אימון ובדיקה
#df_2cols:מאגר הנתונים שלנו
train, test = train_test_split(df_2cols, test_size=0.2)#חלוקה ל20 אחוז בדיקה ו80 לאימון
train, val = train_test_split(train, test_size=0.2)#חלוקה של הנתונים לאימון, כך ש20 אחוז לישמש לאימות ו80 לאימון
#בעצם 20 אחוז משמש לבדיקה, 16 לאימות ו64 לאימון עצמו

!pip install datasets

pip install nltk

!pip install transformers #התקנת טרנספורמרים

from datasets import Dataset, DatasetDict
import pandas as pd

# מניח ש-'df_2cols' הוא הDataFrame המקורי שלך
# חילוק הנתונים לקבוצות של אימון, בדיקה ואימות
train, test = train_test_split(df_2cols, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

# המרה ל־Datasets של Hugging Face
ds_dict = {
    'train': Dataset.from_pandas(train),
    'val': Dataset.from_pandas(val),
    'test': Dataset.from_pandas(test)
}

# יצירת DatasetDict
ds = DatasetDict(ds_dict)

# הדפסת המבנה של DatasetDict
print(ds)

ds = ds.remove_columns("__index_level_0__")# הסרת עמודה על אינדקסים
ds

!pip install transformers #התקנת טרנספורמרים

from transformers import AutoModelForSequenceClassification
from transformers import BertTokenizer

model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

from transformers import BertTokenizer
import tensorflow as tf
!pip install transformers[torch]
!pip install accelerate -U

# ייבא את הטוקנייזר מ-Hugging Face
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)


# בצע טוקניזציה על הטקסט
#tokens = tokenizer(text, padding=True, truncation=True, return_tensors="tf")
tokenized_datasets = ds.map(lambda examples: tokenizer(examples['text'], truncation=True), batched=True)

!pip install transformers[tensorflow] -U

!pip install accelerate[tensorflow] -U

from transformers import AutoTokenizer, DataCollatorWithPadding#ייבוא מחלקות וספריות

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")#הכנה לאופוטימיזציה, ריפוד שיהיה באורך שווה,

pip install transformers accelerate

pip install accelerate>=0.20.1

import accelerate
from transformers import __version__ as transformers_version

print("accelerate version:", accelerate.__version__)
print("transformers version:", transformers_version)

import accelerate
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AdamW
from transformers import TFTrainer, TFTrainingArguments, DataCollatorWithPadding
from transformers import AdamW
from tensorflow.keras.optimizers.schedules import PolynomialDecay
#from accelerate import Accelerator
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AdamW, DataCollatorWithPadding, Trainer, TrainingArguments

accelerator = Accelerator()

# אפשר להמשיך להשתמש ב-imports האחרונים
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

def tokenize_dataset(dataset, tokenizer):   #טוקניזציה
    tokenized_data = []
    for example in dataset:
        tokenized_example = tokenizer(
            example['text'],
            truncation=True,
            padding=True,
            max_length=128,  # You can adjust the max length as needed
            return_tensors='tf'
        )
        tokenized_data.append(tokenized_example)
    return tokenized_data

# Prepare the training dataset
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "token_type_ids", "label_num"],
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator
)

# Prepare the validation dataset
tf_validation_dataset = tokenized_datasets["val"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "label_num"],
    shuffle=False,
    batch_size=4,
    collate_fn=data_collator
)

# Training arguments
training_args = TFTrainingArguments(
    output_dir='',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    evaluation_strategy='epoch',

)


#להבין מה הבעיה עם בהaccelerate
trainer =  TFTrainer(
    model=model,
    args=training_args,
    train_dataset=tf_train_dataset,
    eval_dataset=tf_validation_dataset,

)


#להבין מה הבאג פה
# המשך אימון המודל
trainer.train()

# המשך אימות המודל
results = trainer.evaluate()

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TFTrainer, TFTrainingArguments, DataCollatorWithPadding
from accelerate import Accelerator

# אתה יכול להתחיל את accelerate כך:
accelerator = Accelerator()

# הוא כדאי להשאיר את שורת ה-import של AutoModelForSequenceClassification וגם את השורה של BertTokenizer
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

# Prepare the training dataset
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "token_type_ids", "label_num"],
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator
)

# Prepare the validation dataset
tf_validation_dataset = tokenized_datasets["val"].to_tf_dataset(
    columns=["input_ids", "attention_mask", "label_num"],
    shuffle=False,
    batch_size=4,
    collate_fn=data_collator
)

# Training arguments
training_args = TFTrainingArguments(
    output_dir='',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    evaluation_strategy='epoch'
)

# Trainer initialization
trainer = TFTrainer(
    model=model,
    args=training_args,
    train_dataset=tf_train_dataset,
    eval_dataset=tf_validation_dataset,
    #data_collator=data_collator,
   # tokenizer=tokenizer
)

# Train and evaluate
trainer.train()
results = trainer.evaluate()

from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, AdamW
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from transformers import TFTrainer, TFTrainingArguments
from transformers import AutoTokenizer, DataCollatorWithPadding
from transformers import AdamW
from tensorflow.keras.optimizers.schedules import PolynomialDecay

tokenized_datasets = ds.map(lambda examples: tokenizer(examples['text'], truncation=True), batched=True)

training_args = TFTrainingArguments(
    output_dir='',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    evaluation_strategy='epoch'
)