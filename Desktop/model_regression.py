# -*- coding: utf-8 -*-
"""model_Regrresion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVRAH17X_3bjA9YXigzMcJ6jNp8zKzrf
"""

from google.colab import drive #connect to drive
drive.mount('/content/drive')

! pip install transformers datasets #install

!pip install accelerate -U

# After these installations you need to press RunTime --> Interapt Session!!!
#

import pandas as pd
from datasets import Dataset
from datasets import load_dataset
# Loading the data
df = pd.read_csv('drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/basic_analayze_data/csv_for_hugging_face/sen_6.csv', skiprows=1, names=['Text', 'Age'])
df['Text'] = df['Text'].str.replace('\r', '')
df['Age'] = df['Age'].astype(float)  # convert to float
#mydataset = df["train"].select(range(100)) #If you only want a small sample and not all the data

#Converts the data from Pandas to Hugging Face so we can use it with transformers and natural language processing models
mydataset = Dataset.from_pandas(df)
mydataset.to_pandas()#So that we can present in a convenient way
print(mydataset)

#Only if I want for a sample and not all the data
#import pandas as pd
#from datasets import Dataset

#df = pd.read_csv('drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/basic_analayze_data/csv_for_hugging_face/sen_6.csv', skiprows=1, names=['Text', 'Age'])
#df['Text'] = df['Text'].str.replace('\r', '')
#df['Age'] = df['Age'].astype(float)  # המרה ל-float

#mydataset = Dataset.from_pandas(df)

#mydataset = mydataset.select(range(100))

#print(mydataset)

#library
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorWithPadding, AutoTokenizer  # import AutoTokenizer
from sklearn.model_selection import train_test_split
import numpy as np
from transformers import AutoModelForSequenceClassification
from sklearn.model_selection import train_test_split

# Convert dataset to DataFrame and then to text lists and tags
texts = mydataset.to_pandas()["Text"].tolist()
labels = [float(i) for i in mydataset.to_pandas()["Age"].tolist()]

#divide the data into 3 groups: training, validation and examination. 80% of the data is used for training, 10% for verification, and 10% for the exam.
train_texts, tmp_texts, train_labels, tmp_labels = train_test_split(texts, labels, test_size=0.2)
val_texts, test_texts, val_labels, test_labels = train_test_split(tmp_texts, tmp_labels, test_size=0.5)

#loads the BERT tokenizer, which will be used to tokenize (decompose into tokens) the texts
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

#perform tokenization of the texts in each of the groups (training, verification and examination), while shortening and padding to adjust the texts to a fixed length
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# defines a class that creates a PyTorch dataset from the token and label arrays.
class TextClassificationDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        #item['labels'] = torch.tensor(self.labels[idx])
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.labels)


#create dataset objects for each of the groups: training, validation and examination
train_dataset = TextClassificationDataset(train_encodings, train_labels)
val_dataset = TextClassificationDataset(val_encodings, val_labels)
test_dataset = TextClassificationDataset(test_encodings, test_labels)

# data collator, divide to batch, doing paddin, pcking the data after padding - Basically everything is technical
data_collator = DataCollatorWithPadding(tokenizer)

#print(train_dataset[0]) #print example

#This line imports the AutoModelForSequenceClassification model from Hugging Face's Transformers library. This is a ready-made transformer model for classifying sequences (ie, textual input)
from transformers import AutoModelForSequenceClassification

#This line loads the base BERT model (bert-base-uncased) and fits it to a one-dimensional regression problem (ie, predicting one numerical value)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", problem_type="regression", num_labels=1)

#"bert-base-uncased": This is the name of the preloaded transformer model. Basic BERT is an efficient and common transformer model for natural language processing.
#problem_type="regression": indicates that this is a regression problem and not a classification one
#num_labels=1: indicates that the goal is to predict one numeric value (in this case, the age)

from transformers import TrainingArguments


training_args = TrainingArguments(
    output_dir="drive/MyDrive/data_sets/Bert_Cola/test_trainer/",  # output directory
    overwrite_output_dir=True,
    learning_rate=2e-5, # the learning rate for optimizing the weights during training for BERT
    per_device_train_batch_size=16, #size of batch
    per_device_eval_batch_size=16,#size of batch
    num_train_epochs=15, # number of epochs to train the model
    weight_decay=0.01, #Normalizing the weights, שיניתי מ0.01
    eval_strategy="no",#Evaluation at the end of each epoch
    save_strategy="no",#save at the end of each epoch
    load_best_model_at_end=True,#Do load the best model (with the highest performance) at the end of training.
    push_to_hub=False, #don't push the model to the hub
    logging_steps =20 #logging steps
)

import numpy as np
from datasets import load_metric
import torch.nn.functional as F

metric = load_metric("mse")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.squeeze()
    mse = ((predictions - labels) ** 2).mean().item()
    return {"mse": mse}

import torch.nn.functional as F
from transformers import Trainer
import torch
from transformers import Trainer

class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(-1)
        loss_fn = F.mse_loss
        loss = loss_fn(logits, labels)
        return (loss, outputs) if return_outputs else loss

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    #eval_dataset=val_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# save model
model.save_pretrained("drive/MyDrive")

# These lines run the trained model on the validation set
val_predictions = trainer.predict(val_dataset)
val_preds = val_predictions.predictions.squeeze()

# חישוב מדדי רגרסיה על מערך האימות
val_true = val_dataset.labels
val_mse = mean_squared_error(val_true, val_preds)#The average of the squared errors between the predicted values ​​and the true values, like variance in statistic
val_rmse = np.sqrt(val_mse) #like standart deviation
val_mae = mean_absolute_error(val_true, val_preds) #The average of the absolute values ​​of the errors between the values, similar to MSE
val_r2 = r2_score(val_true, val_preds) #correlation

#ההבדל בין MSE וMAE :
#MSE ו-RMSE מודדים את גודל השגיאות הממוצעות, וערכים נמוכים טובים יותר.
#MAE מודד את הסטייה המוחלטת הממוצעת, וערך נמוך טוב יותר

print(f"Validation Mean Squared Error (MSE): {val_mse:.4f}")
print(f"Validation Root Mean Squared Error (RMSE): {val_rmse:.4f}")
print(f"Validation Mean Absolute Error (MAE): {val_mae:.4f}")
print(f"Validation R² (Coefficient of Determination): {val_r2:.4f}")

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# שמירת המודל
model.save_pretrained("drive/MyDrive")


test_predictions = trainer.predict(test_dataset)
test_preds = test_predictions.predictions.squeeze()

# חישוב מדדי רגרסיה על מערך הבחינה
test_true = test_dataset.labels
test_mse = mean_squared_error(test_true, test_preds)
test_rmse = np.sqrt(test_mse)
test_mae = mean_absolute_error(test_true, test_preds)
test_r2 = r2_score(test_true, test_preds)

print(f"Test Mean Squared Error (MSE): {test_mse:.4f}")
print(f"Test Root Mean Squared Error (RMSE): {test_rmse:.4f}")
print(f"Test Mean Absolute Error (MAE): {test_mae:.4f}")
print(f"Test R² (Coefficient of Determination): {test_r2:.4f}")