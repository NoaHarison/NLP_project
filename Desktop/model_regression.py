# -*- coding: utf-8 -*-
"""model_Regrresion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kVRAH17X_3bjA9YXigzMcJ6jNp8zKzrf
"""

from google.colab import drive # Connect to Google Drive
drive.mount('/content/drive')

! pip install transformers datasets # Install required libraries

!pip install accelerate -U # Install Accelerate library

# After these installations you need to press RunTime --> Interrupt Session!!!

import pandas as pd
from datasets import Dataset
from datasets import load_dataset

# Loading the data from a CSV file in Google Drive
df = pd.read_csv('drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/basic_analayze_data/csv_for_hugging_face/sen_6.csv', skiprows=1, names=['Text', 'Age'])
df['Text'] = df['Text'].str.replace('\r', '') # Clean text data
df['Age'] = df['Age'].astype(float)  # Convert Age column to float

# Convert the DataFrame to a Hugging Face Dataset for compatibility with Transformers
mydataset = Dataset.from_pandas(df)
mydataset.to_pandas() # Convert back to Pandas DataFrame to display data
print(mydataset)

# Import necessary libraries for model creation and training
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments
from transformers import DataCollatorWithPadding, AutoTokenizer  # Import AutoTokenizer
from sklearn.model_selection import train_test_split
import numpy as np
from transformers import AutoModelForSequenceClassification

# Convert dataset to lists for further processing
texts = mydataset.to_pandas()["Text"].tolist()
labels = [float(i) for i in mydataset.to_pandas()["Age"].tolist()]

# Split data into train, validation, and test sets (80% train, 10% validation, 10% test)
train_texts, tmp_texts, train_labels, tmp_labels = train_test_split(texts, labels, test_size=0.2)
val_texts, test_texts, val_labels, test_labels = train_test_split(tmp_texts, tmp_labels, test_size=0.5)

# Load the BERT tokenizer for tokenizing text data
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Perform tokenization for train, validation, and test datasets
train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

# Define a custom dataset class for PyTorch
class TextClassificationDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

    def __len__(self):
        return len(self.labels)

# Create dataset objects for train, validation, and test datasets
train_dataset = TextClassificationDataset(train_encodings, train_labels)
val_dataset = TextClassificationDataset(val_encodings, val_labels)
test_dataset = TextClassificationDataset(test_encodings, test_labels)

# Prepare data collator for padding and batching
data_collator = DataCollatorWithPadding(tokenizer)

# Load the pre-trained BERT model and configure it for regression (predicting a continuous value)
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", problem_type="regression", num_labels=1)

# Define training arguments, including the learning rate, batch size, and number of epochs
training_args = TrainingArguments(
    output_dir="drive/MyDrive/data_sets/Bert_Cola/test_trainer/",  # Output directory for saved model
    overwrite_output_dir=True,
    learning_rate=2e-5,  # Learning rate for optimization
    per_device_train_batch_size=16,  # Batch size for training
    per_device_eval_batch_size=16,  # Batch size for evaluation
    num_train_epochs=15,  # Number of training epochs
    weight_decay=0.01,  # Weight decay (regularization)
    eval_strategy="no",  # Evaluation strategy
    save_strategy="no",  # Saving strategy
    load_best_model_at_end=True,  # Load the best model at the end of training
    push_to_hub=False,  # Do not push the model to Hugging Face Hub
    logging_steps=20  # Log training progress every 20 steps
)

import numpy as np
from datasets import load_metric
import torch.nn.functional as F

metric = load_metric("mse")  # Load mean squared error (MSE) metric

# Function to compute evaluation metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.squeeze()
    mse = ((predictions - labels) ** 2).mean().item()  # Calculate MSE
    return {"mse": mse}

# Define a custom trainer class to compute loss using MSE
class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits.squeeze(-1)
        loss_fn = F.mse_loss
        loss = loss_fn(logits, labels)  # Compute loss using mean squared error
        return (loss, outputs) if return_outputs else loss

# Initialize the trainer with the model, training arguments, and datasets
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Save the trained model to Google Drive
model.save_pretrained("drive/MyDrive")

# Run the trained model on the validation dataset and calculate regression metrics
val_predictions = trainer.predict(val_dataset)
val_preds = val_predictions.predictions.squeeze()

val_true = val_dataset.labels
val_mse = mean_squared_error(val_true, val_preds)  # Mean squared error
val_rmse = np.sqrt(val_mse)  # Root mean squared error
val_mae = mean_absolute_error(val_true, val_preds)  # Mean absolute error
val_r2 = r2_score(val_true, val_preds)  # R-squared (coefficient of determination)

# Print validation metrics
print(f"Validation Mean Squared Error (MSE): {val_mse:.4f}")
print(f"Validation Root Mean Squared Error (RMSE): {val_rmse:.4f}")
print(f"Validation Mean Absolute Error (MAE): {val_mae:.4f}")
print(f"Validation R² (Coefficient of Determination): {val_r2:.4f}")

# Run the trained model on the test dataset and calculate regression metrics
test_predictions = trainer.predict(test_dataset)
test_preds = test_predictions.predictions.squeeze()

test_true = test_dataset.labels
test_mse = mean_squared_error(test_true, test_preds)
test_rmse = np.sqrt(test_mse)
test_mae = mean_absolute_error(test_true, test_preds)
test_r2 = r2_score(test_true, test_preds)

# Print test metrics
print(f"Test Mean Squared Error (MSE): {test_mse:.4f}")
print(f"Test Root Mean Squared Error (RMSE): {test_rmse:.4f}")
print(f"Test Mean Absolute Error (MAE): {test_mae:.4f}")
print(f"Test R² (Coefficient of Determination): {test_r2:.4f}")
