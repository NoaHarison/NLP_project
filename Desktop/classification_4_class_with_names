# -*- coding: utf-8 -*-
"""bert_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1spbQhPTzkIO8iYbkdJFYUmg-sNTJu4_3
"""

# ================================================
# 📚 ייבוא ספריות
# ================================================

import pandas as pd
import numpy as np
import torch
import ast

from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, f1_score
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback
import re
import wandb
wandb.init(mode='disabled')

# ================================================
# 📂 טעינת הדאטה
# ================================================

# שימי לב: תשני את הנתיב למיקום שבו שמרת את הקובץ בשרת
df = pd.read_csv("/workspace/clean_kids_Dataset_Ver01.csv")

# תיוג של המחלקות
age_bin_map = {
    "24-35": 0,
    "36-47": 1,
    "48-59": 2,
    "60-71": 3
}
df["age_bin_label"] = df["age_bin"].map(age_bin_map)

# ערבוב נתונים
df = df.sample(frac=1, random_state=42).reset_index(drop=True)

# ================================================
# ✂️ חילוץ טקסטים של ילדים בלבד
# ================================================


def extract_and_clean_child_utterances(convo_str):
    try:
        convo = ast.literal_eval(convo_str)
        child_utterances = [
            utt.strip() + "."
            for speaker, utt in convo
            if isinstance(utt, str) and speaker.strip().upper() == "CHI"
        ]
        text = " ".join(child_utterances)

        # הסרת המילים unrecognized_word (לא תלוי רישיות)
        text = re.sub(r'\bunrecognized_word\b', '', text, flags=re.IGNORECASE)

        # הסרת סימני קריאה, שאלה, מירכאות ונקודה (נקודה נשמרת רק בסוף משפטים שנוספו קודם)
        text = re.sub(r'[!?,\"…]', '', text)

        # המרה לאותיות קטנות
        text = text.lower()

        # הסרת רווחים מיותרים
        text = re.sub(r'\s+', ' ', text).strip()

        return text
    except (ValueError, SyntaxError):
        return ""


df["child_text"] = df["conversation"].apply(extract_and_clean_child_utterances)


# משתנים לפיצול
X = df["child_text"]
y = df["age_bin_label"]
groups = df["child_name"]

# ================================================
# 🔥 מודל RoBERTa
# ================================================

# טוקניזר
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_texts(texts):
    return tokenizer(
        list(texts),
        padding=True,
        truncation=True,
        max_length=256,
        return_tensors="pt"
    )

# מחלקת Dataset
class ChildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

    def __len__(self):
        return len(self.labels)

# פונקציה לחישוב מטריקות
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    return {
        'f1': f1_score(p.label_ids, preds, average='weighted')
    }

# ================================================
# 🔁 Cross Validation
# ================================================

gkf = StratifiedGroupKFold(n_splits=5)
f1_scores = []

for fold, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups)):
    print(f"\n🚀 Fold {fold + 1}")

    # טוקניזציה
    train_encodings = tokenize_texts(X.iloc[train_idx])
    test_encodings = tokenize_texts(X.iloc[test_idx])

    # תוויות
    train_labels = torch.tensor(y.iloc[train_idx].values)
    test_labels = torch.tensor(y.iloc[test_idx].values)

    # דייטאסטים
    train_dataset = ChildDataset(train_encodings, train_labels)
    eval_dataset = ChildDataset(test_encodings, test_labels)

    # מודל
    model = RobertaForSequenceClassification.from_pretrained(
        'roberta-base',
        num_labels=4
    )

    training_args = TrainingArguments(
        output_dir=f'./results_fold_{fold}',
        learning_rate=2e-5,
        num_train_epochs=10,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=32,
        evaluation_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        logging_dir='./logs',
        logging_steps=10,
        load_best_model_at_end=True,
        weight_decay=0.01,
        metric_for_best_model="f1",
        greater_is_better=True,
    )

    early_stopping = EarlyStoppingCallback(early_stopping_patience=2)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics,
        callbacks=[early_stopping],
    )

    # אימון
    trainer.train()

    # ניבוי
    preds = trainer.predict(eval_dataset)
    preds_labels = np.argmax(preds.predictions, axis=1)

    # הערכה
    print(classification_report(test_labels.numpy(), preds_labels))
    f1 = f1_score(test_labels.numpy(), preds_labels, average="weighted")
    f1_scores.append(f1)

# ================================================
# 📈 תוצאה סופית
# ================================================

print("\n✅ Average weighted F1-score across folds:", np.mean(f1_scores))




# ================================================
#     precision    recall  f1-score   support
#
#          0       0.77      0.78      0.78       105
#         1       0.51      0.48      0.49        96
#          2       0.46      0.53      0.50        73
#           3       0.78      0.72      0.75        92
#
#   accuracy                           0.64       366
#   macro avg       0.63      0.63      0.63       366
#weighted avg       0.64      0.64      0.64       366
#
#
#✅ Average weighted F1-score across folds: 0.6541427593750229
# ================================================
