# -*- coding: utf-8 -*-
"""spam_or_not_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1knoiB3hmqZSfKBCVXDzppDWNqkCUFVML
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

!apt-get install -y git
!git config --global credential.helper 'cache --timeout=3600'
!git clone https://github.com/שם_משתמש/שם_הפרויקט.git
!git add .
!git config --global user.email "your.email@example.com"
!git config --global user.name "Your Name"
!git commit -m "תיאור של השינויים שביצעת"
!git push origin master

# Import libraries
import numpy as np
import pandas as pd
import re
import tensorflow as tf
from tensorflow.keras.layers import Dense, Input, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow_hub as hub
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import BertTokenizer, AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, f1_score
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers.schedules import PolynomialDecay
from tensorflow.keras.optimizers import Adam

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Load and shuffle dataset
df = pd.read_csv('drive/MyDrive/bioinformatica/Colab_Notebooks/final_project/spam_or_not/spam_ham_dataset.csv', names=['num', 'label', 'text', 'label_num'])
df = shuffle(df)

# Convert 'label_num' to numeric, handle NaN values, and convert to integers
df['label_num'] = pd.to_numeric(df['label_num'], errors='coerce')
df['label_num'] = df['label_num'].fillna(0).astype(int)

# Create a DataFrame with only 'label_num' and 'text' columns
df_2cols = df[['label_num', 'text']]

# Extract text and labels
text = df.text.values
labels = df.label_num.values

# Display label counts
print(df.label_num.value_counts())

# Plot value counts
plt.bar(df.label_num.value_counts().index, df.label_num.value_counts().values)
plt.xlabel('Categories')
plt.ylabel('Counts')
plt.title('Value Counts Chart')
plt.show()

# Add a new column 'text_length' containing the length of each text
df['text_length'] = df['text'].apply(len)

# Create a histogram of text lengths
plt.hist(df['text_length'], bins=20, color='blue', edgecolor='black')
plt.xlabel('Text Length')
plt.ylabel('Frequency')
plt.title('Histogram of Text Length')
plt.show()

# Split the dataset into training, validation, and testing sets
train, test = train_test_split(df_2cols, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

# Install datasets library
!pip install datasets

# Convert the splits to Pandas DataFrames
train, test, val = pd.DataFrame(train), pd.DataFrame(test), pd.DataFrame(val)

# Create DatasetDict
ds_dict = {'train': Dataset.from_pandas(train), 'val': Dataset.from_pandas(val), 'test': Dataset.from_pandas(test)}
ds = DatasetDict(ds_dict)

# Remove unnecessary column
ds = ds.remove_columns("__index_level_0__")

# Install transformers library
!pip install transformers

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Tokenize function
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True)

# Apply tokenization
tokenized_datasets = ds.map(tokenize_function, batched=True)

# Data collator with padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

# Convert datasets to TensorFlow datasets
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["label_num"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=4,
)

tf_validation_dataset = tokenized_datasets["val"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["label_num"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=4,
)

tf_test_dataset = tokenized_datasets["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["label_num"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=4,
)

# Prepare the dataset for TensorFlow
def prepare_dataset(dataset):
    input_ids = tf.convert_to_tensor(dataset["input_ids"])
    attention_mask = tf.convert_to_tensor([x if x is not None else [1] * len(dataset["input_ids"][0]) for x in dataset["attention_mask"]])
    token_type_ids = tf.convert_to_tensor(dataset["token_type_ids"])
    labels = tf.convert_to_tensor(dataset["label_num"])
    return tf.data.Dataset.from_tensor_slices(({"input_ids": input_ids,
                                                 "attention_mask": attention_mask,
                                                 "token_type_ids": token_type_ids},
                                                labels))

# Load pre-trained BERT model for sequence classification
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Compile the model with Adam optimizer and sparse categorical cross-entropy loss
model.compile(optimizer="adam", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])

# Train the model
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=5)

# Adjust learning rate schedule
batch_size = 15
num_epochs = 5
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps)
opt = Adam(learning_rate=lr_scheduler)

# Compile the model with the new optimizer and loss
model.compile(optimizer=opt, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=["accuracy"])

# Train the model again with the new optimizer
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=num_epochs)

# Make predictions on the test set
preds = model.predict(tf_test_dataset)["logits"]
class_preds = np.argmax(preds, axis=1)

# Print the accuracy and F1 scores
print(accuracy_score(test.label_num, class_preds))
print(f1_score(test.label_num, class_preds))
